{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a714e10e",
   "metadata": {},
   "source": [
    "## Classical Flow Matching\n",
    "Implementing a classical flow matching model to compare to the quantum model, which ended up being diffusion/flow matching inspired.\n",
    "\n",
    "Heavily inspired by: [Flow Matching on MNIST](https://github.com/seichang00/flow-matching-lightning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "208d5b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\realc\\OneDrive\\Documents\\GSOC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Users/realc/OneDrive/Documents/GSOC\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d16d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "out_dir = \"fm_outputs\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "image_size = 28\n",
    "channels = 1\n",
    "latent_std = 1.0  # standard deviation for gaussian prior\n",
    "save_every = 10\n",
    "subset_size = 10000  # Set to None to use full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10f887ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),               # 0-1\n",
    "    transforms.Normalize((0.5,), (0.5,)) # map to [-1, 1]\n",
    "])\n",
    "train_ds = datasets.MNIST(root=\"./mnist\", train=True, download=False, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
    "\n",
    "if subset_size is not None:\n",
    "    train_ds = torch.utils.data.Subset(train_ds, range(subset_size))\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cf153b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(dim, dim*4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim*4, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # sinusoidal embedding (like transformer / diffusion)\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device) / half)\n",
    "        args = t.unsqueeze(-1) * freqs.unsqueeze(0)  # (B, half)\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        return self.lin(emb)\n",
    "    \n",
    "class SmallConvField(nn.Module):\n",
    "    def __init__(self, in_ch=1, base_ch=64, time_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.time_emb = TimeEmbedding(time_emb_dim)\n",
    "\n",
    "        self.init_conv = nn.Conv2d(in_ch + 1, base_ch, kernel_size=3, padding=1) # +1 channel for time broadcast\n",
    "        self.down1 = nn.Conv2d(base_ch, base_ch, kernel_size=3, padding=1)\n",
    "        self.down2 = nn.Conv2d(base_ch, base_ch, kernel_size=3, padding=1)\n",
    "\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(base_ch, base_ch, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch, base_ch, kernel_size=3, padding=1),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        # inject time embedding to final layers via broadcasting and addition\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(base_ch, base_ch, kernel_size=3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_ch, in_ch, kernel_size=3, padding=1)  # predict same-channel vector field\n",
    "        )\n",
    "\n",
    "        self.time_proj = nn.Linear(time_emb_dim, base_ch)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W) where C=1 (image)\n",
    "        t: (B,) in [0,1]\n",
    "        returns v(x,t) shape same as x\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        # broadcast time as an extra channel (scalar per image)\n",
    "        t_channel = t.view(B, 1, 1, 1).expand(-1, 1, H, W)\n",
    "        xt = torch.cat([x, t_channel], dim=1)  # (B, C+1, H, W)\n",
    "        h = self.init_conv(xt)\n",
    "        h = F.silu(self.down1(h) + h)\n",
    "        h = F.silu(self.down2(h) + h)\n",
    "\n",
    "        h = self.mid(h)\n",
    "        # time embedding projection and add\n",
    "        t_emb = self.time_emb(t)             # (B, time_emb_dim)\n",
    "        t_proj = self.time_proj(t_emb)       # (B, base_ch)\n",
    "        t_proj = t_proj.view(B, -1, 1, 1)    # (B, base_ch, 1, 1)\n",
    "        h = h + t_proj\n",
    "\n",
    "        out = self.final(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3353a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_mix(x0, x1, t):\n",
    "    # x_t = (1 - t) * x0 + t * x1\n",
    "    return (1.0 - t.view(-1,1,1,1)) * x0 + t.view(-1,1,1,1) * x1\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    for imgs, _ in pbar:\n",
    "        imgs = imgs.to(device)  # x0: (B,1,28,28) in [-1,1]\n",
    "        B = imgs.shape[0]\n",
    "\n",
    "        # sample random prior images x1 ~ N(0, I)\n",
    "        x1 = torch.randn_like(imgs) * latent_std\n",
    "\n",
    "        # sample t uniformly in [0,1)\n",
    "        t = torch.rand(B, device=device)\n",
    "\n",
    "        # construct x_t\n",
    "        x_t = linear_mix(imgs, x1, t)\n",
    "\n",
    "        # target vector field: dx_t/dt = x1 - x0 (constant for linear interpolation)\n",
    "        target_v = (x1 - imgs)\n",
    "\n",
    "        # predict v(x_t, t)\n",
    "        pred_v = model(x_t, t)\n",
    "\n",
    "        loss = F.mse_loss(pred_v, target_v)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * B\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def rk4_integration(model, x_init, t0=1.0, t1=0.0, steps=50):\n",
    "    \"\"\"\n",
    "    Integrate dx/dt = v(x, t) from t0 -> t1 using RK4\n",
    "    x_init: (B, C, H, W) initial states at time t0\n",
    "    returns x_final at t1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = x_init.clone()\n",
    "    B = x.shape[0]\n",
    "    t0 = float(t0)\n",
    "    t1 = float(t1)\n",
    "    dt = (t1 - t0) / steps  # negative if t1 < t0\n",
    "\n",
    "    t = t0\n",
    "    for i in range(steps):\n",
    "        t_tensor = torch.full((B,), t, device=x.device)\n",
    "        k1 = model(x, t_tensor)\n",
    "        k2 = model(x + 0.5 * dt * k1, t_tensor + 0.5 * dt)\n",
    "        k3 = model(x + 0.5 * dt * k2, t_tensor + 0.5 * dt)\n",
    "        k4 = model(x + dt * k3, t_tensor + dt)\n",
    "        x = x + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "        t = t + dt\n",
    "    return x\n",
    "\n",
    "def sample_and_save(model, epoch, n=64, steps=80):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n, channels, image_size, image_size, device=device) * latent_std  # sample from prior at t=1\n",
    "        x = rk4_integration(model, z, t0=1.0, t1=0.0, steps=steps)\n",
    "        # map back from [-1,1] to [0,1] for visualization\n",
    "        imgs = (x.clamp(-1,1) + 1.0) / 2.0\n",
    "        grid = utils.make_grid(imgs, nrow=int(math.sqrt(n)), pad_value=1.0)\n",
    "        save_path = os.path.join(out_dir, f\"samples_epoch_{epoch}.png\")\n",
    "        utils.save_image(grid, save_path)\n",
    "        print(f\"Saved samples to {save_path}\")\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238d9f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  12%|█▎        | 39/312 [00:17<02:03,  2.21it/s, loss=0.609]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     loss_history\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m average loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(pred_v, target_v)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     34\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m B\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SmallConvField(in_ch=channels, base_ch=64, time_emb_dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, epoch)\n",
    "    loss_history.append(train_loss)\n",
    "    print(f\"Epoch {epoch} average loss: {train_loss:.6f}\")\n",
    "\n",
    "    if epoch % save_every == 0 or epoch == epochs:\n",
    "        sample_and_save(model, epoch, n=64, steps=100)\n",
    "        torch.save(model.state_dict(), os.path.join(out_dir, f\"model_epoch_{epoch}.pth\"))\n",
    "        print(\"Saved model checkpoint.\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, len(loss_history)+1), loss_history, marker='o')\n",
    "plt.title(\"Flow Matching Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45643b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
