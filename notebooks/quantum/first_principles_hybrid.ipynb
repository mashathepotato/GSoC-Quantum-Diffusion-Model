{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "397a3d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/masha/Documents/GSOC/GSoC-Quantum-Diffusion-Model\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/masha/Documents/GSOC/GSoC-Quantum-Diffusion-Model\n",
    "\n",
    "from utils.post_training import *\n",
    "from utils.statistics import *\n",
    "from utils.plotting import *\n",
    "from utils.angle_encoding_script import angle_encoding\n",
    "from utils.haar_noising_script import apply_haar_scrambling\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim_func\n",
    "\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90a702fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "[[-1.       -1.       -1.       ... -1.       -1.       -1.      ]\n",
      " [-1.       -1.       -1.       ... -1.       -1.       -1.      ]\n",
      " [-1.       -1.       -1.       ... -0.989879 -1.       -1.      ]\n",
      " ...\n",
      " [-1.       -1.       -1.       ... -1.       -1.       -1.      ]\n",
      " [-1.       -1.       -1.       ... -1.       -1.       -1.      ]\n",
      " [-1.       -1.       -1.       ... -1.       -1.       -1.      ]]\n",
      "tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.9999, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])\n",
      "Data Shape: (1000, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "filename = \"data/QG1_64x64_1k\"\n",
    "data_X = np.array(h5py.File(filename, \"r\")['X'])\n",
    "data_X = np.log1p(data_X.astype(np.float32))\n",
    "data_X = data_X / data_X.max()\n",
    "data_X = 2.0 * data_X - 1.0 # Range [-1, 1]\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=1)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def pixel_circuit(inputs):\n",
    "    thetas = (inputs + 1.0) * (np.pi / 2.0)\n",
    "    \n",
    "    qml.RY(thetas, wires=0)\n",
    "    \n",
    "    # Measure Z expectation\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def embed_pixels_flat(images):\n",
    "    N, H, W = images.shape\n",
    "\n",
    "    # Flatten everything: (N * 64 * 64)\n",
    "    flat_pixels = torch.tensor(images).view(-1)\n",
    "        \n",
    "    # Big batch size if circuit is small\n",
    "    batch_size = 16384 \n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(flat_pixels), batch_size):\n",
    "            batch = flat_pixels[i : i + batch_size]\n",
    "            q_out = pixel_circuit(batch)\n",
    "            results.append(q_out)\n",
    "            \n",
    "    results = torch.cat(results, dim=0)\n",
    "    \n",
    "    q_images = results.reshape(N, 1, H, W)\n",
    "    return q_images.float()\n",
    "\n",
    "print(data_X[0])\n",
    "quantum_data = embed_pixels_flat(data_X)\n",
    "print(quantum_data[0])\n",
    "\n",
    "print(f\"Data Shape: {data_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "557bf41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.942388\n",
      "Epoch 2: 0.285077\n",
      "Epoch 4: 0.007307\n",
      "Epoch 6: 0.000527\n",
      "Epoch 8: 0.000019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PixelDecoder(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PixelDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Learnable inverse function\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, 1) # Output 1 channel\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "decoder = PixelDecoder().to(device)\n",
    "opt_dec = optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "crit_dec = nn.MSELoss()\n",
    "\n",
    "dset_dec = TensorDataset(quantum_data.to(device), torch.tensor(data_X).unsqueeze(1).to(device))\n",
    "loader_dec = DataLoader(dset_dec, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for q, real in loader_dec:\n",
    "        pred = decoder(q)\n",
    "        loss = crit_dec(pred, real)\n",
    "        opt_dec.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_dec.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch}: {total_loss/len(loader_dec):.6f}\")\n",
    "\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "626f6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.lin = nn.Sequential(nn.Linear(dim, dim), nn.SiLU(), nn.Linear(dim, dim))\n",
    "    def forward(self, t):\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=t.device) / half)\n",
    "        args = t.unsqueeze(-1) * freqs.unsqueeze(0)\n",
    "        return self.lin(torch.cat([torch.sin(args), torch.cos(args)], dim=-1))\n",
    "\n",
    "class CoordUNet(nn.Module):\n",
    "    def __init__(self, in_ch=4):\n",
    "        super().__init__()\n",
    "        self.time_emb = TimeEmbedding(64)\n",
    "        \n",
    "        # In: 4 quantum + 2 coord = 6\n",
    "        self.inc = nn.Conv2d(in_ch + 2, 64, 3, padding=1)\n",
    "        self.down1 = nn.Sequential(nn.Conv2d(64, 128, 4, 2, 1), nn.SiLU())\n",
    "        self.down2 = nn.Sequential(nn.Conv2d(128, 256, 4, 2, 1), nn.SiLU())\n",
    "        self.mid = nn.Sequential(nn.Conv2d(256, 256, 3, 1, 1), nn.SiLU())\n",
    "        \n",
    "        self.t_mlp1 = nn.Linear(64, 128)\n",
    "        self.t_mlp2 = nn.Linear(64, 256)\n",
    "        \n",
    "        self.up1 = nn.Sequential(nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.SiLU())\n",
    "        self.up2 = nn.Sequential(nn.ConvTranspose2d(256, 64, 4, 2, 1), nn.SiLU())\n",
    "        self.outc = nn.Conv2d(64, in_ch, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, C, H, W = x.shape\n",
    "        # Add Coordinates\n",
    "        yy, xx = torch.meshgrid(torch.linspace(-1,1,H,device=x.device), torch.linspace(-1,1,W,device=x.device), indexing='ij')\n",
    "        x_in = torch.cat([x, yy.expand(B,1,H,W), xx.expand(B,1,H,W)], dim=1)\n",
    "        \n",
    "        t_emb = self.time_emb(t)\n",
    "        x1 = F.silu(self.inc(x_in))\n",
    "        x2 = self.down1(x1) + self.t_mlp1(t_emb)[...,None,None]\n",
    "        x3 = self.down2(x2) + self.t_mlp2(t_emb)[...,None,None]\n",
    "        mid = self.mid(x3)\n",
    "        u1 = self.up1(mid)\n",
    "        u1 = torch.cat([u1, x2], dim=1)\n",
    "        u2 = self.up2(u1)\n",
    "        return self.outc(u2 + x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d64daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pixel-Wise Flow Matching...\n",
      "Epoch 10: Loss 0.22566\n"
     ]
    }
   ],
   "source": [
    "class CoordUNet(nn.Module):\n",
    "    def __init__(self, in_ch=1): # Changed to 1 channel\n",
    "        super().__init__()\n",
    "        self.time_emb = TimeEmbedding(64)\n",
    "        \n",
    "        # In: 1 quantum + 2 coord = 3\n",
    "        self.inc = nn.Conv2d(in_ch + 2, 64, 3, padding=1)\n",
    "        self.down1 = nn.Sequential(nn.Conv2d(64, 128, 4, 2, 1), nn.SiLU())\n",
    "        self.down2 = nn.Sequential(nn.Conv2d(128, 256, 4, 2, 1), nn.SiLU())\n",
    "        self.mid = nn.Sequential(nn.Conv2d(256, 256, 3, 1, 1), nn.SiLU())\n",
    "        \n",
    "        self.t_mlp1 = nn.Linear(64, 128)\n",
    "        self.t_mlp2 = nn.Linear(64, 256)\n",
    "        \n",
    "        self.up1 = nn.Sequential(nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.SiLU())\n",
    "        self.up2 = nn.Sequential(nn.ConvTranspose2d(256, 64, 4, 2, 1), nn.SiLU())\n",
    "        self.outc = nn.Conv2d(64, in_ch, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, C, H, W = x.shape\n",
    "        yy, xx = torch.meshgrid(torch.linspace(-1,1,H,device=x.device), torch.linspace(-1,1,W,device=x.device), indexing='ij')\n",
    "        x_in = torch.cat([x, yy.expand(B,1,H,W), xx.expand(B,1,H,W)], dim=1)\n",
    "        \n",
    "        t_emb = self.time_emb(t)\n",
    "        x1 = F.silu(self.inc(x_in))\n",
    "        x2 = self.down1(x1) + self.t_mlp1(t_emb)[...,None,None]\n",
    "        x3 = self.down2(x2) + self.t_mlp2(t_emb)[...,None,None]\n",
    "        mid = self.mid(x3)\n",
    "        u1 = self.up1(mid)\n",
    "        u1 = torch.cat([u1, x2], dim=1)\n",
    "        u2 = self.up2(u1)\n",
    "        return self.outc(u2 + x1)\n",
    "\n",
    "# Training Loop\n",
    "def robust_weighted_loss(pred, target, x1_data, weight_factor=10.0):\n",
    "    # x1_data is now 1 channel. Signal is ~ -1. Background ~ +1.\n",
    "    importance = (1.0 - x1_data) * 0.5\n",
    "    weights = 1.0 + (weight_factor * importance)\n",
    "    return ((pred - target)**2 * weights).mean()\n",
    "\n",
    "model = CoordUNet(in_ch=1).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loader = DataLoader(TensorDataset(quantum_data.to(device)), batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Training Pixel-Wise Flow Matching...\")\n",
    "losses = []\n",
    "for epoch in range(1, 41):\n",
    "    model.train()\n",
    "    batch_loss = 0\n",
    "    for (x1,) in loader:\n",
    "        x0 = torch.randn_like(x1)\n",
    "        t = torch.rand(x1.size(0), device=device)\n",
    "        t_view = t.view(-1, 1, 1, 1)\n",
    "        x_t = (1-t_view)*x0 + t_view*x1\n",
    "        target_v = x1 - x0\n",
    "        \n",
    "        pred_v = model(x_t, t)\n",
    "        loss = robust_weighted_loss(pred_v, target_v, x1)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        batch_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {batch_loss/len(loader):.5f}\")\n",
    "        losses.append(batch_loss/len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb758d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
