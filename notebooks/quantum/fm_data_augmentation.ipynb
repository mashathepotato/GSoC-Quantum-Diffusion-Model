{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b03dab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\realc\\OneDrive\\Documents\\GSOC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/masha/Documents/GSOC/GSoC-Quantum-Diffusion-Model\n",
    "\n",
    "from utils.post_training import *\n",
    "from utils.statistics import *\n",
    "from utils.plotting import *\n",
    "from utils.encodings import *\n",
    "from utils.statistics import calculate_statistics, calculate_fid, ssim\n",
    "from utils.haar_noising_script import apply_global_haar_scrambling, fast_haar_scramble\n",
    "from utils.quantum_diffusion import *\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.linalg\n",
    "import random\n",
    "\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "419104d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "QG_channel = 1\n",
    "filename = f\"data/QG{QG_channel}_64x64_1k\"\n",
    "num_train_samples = None\n",
    "num_samples_for_scramble = 100   # how many samples to produce scrambled versions for\n",
    "num_scrambles = 4   # number of scrambled augmentations per sample\n",
    "n_qubits = 12\n",
    "scramble_depth = 8\n",
    "shared_unitaries = True\n",
    "batch_size = 16\n",
    "lr = 2e-4\n",
    "num_epochs = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# output directories\n",
    "out_dir = \"results_qg_cond_fm\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b849d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (1000, 64, 64)\n",
      "Prepared data shape: (1000, 1, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(filename, \"r\")\n",
    "data_X = np.array(f['X'])\n",
    "print(\"Raw data shape:\", data_X.shape)  # expect (N, H, W) or (N, 1, H, W)\n",
    "\n",
    "if data_X.ndim == 4:\n",
    "    # (N, C, H, W)\n",
    "    data = data_X\n",
    "else:\n",
    "    # (N, H, W) -> (N, 1, H, W)\n",
    "    data = data_X[:, None, :, :]\n",
    "\n",
    "# optional subsample for faster debug runs\n",
    "if num_train_samples is not None:\n",
    "    data = data[:num_train_samples]\n",
    "\n",
    "print(\"Prepared data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081558cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a set of 2 shared unitaries\n",
      "encoded shape: (10, 4096) scrambles samples: 10\n"
     ]
    }
   ],
   "source": [
    "# Use the generate_scrambled_dataset you already provided, but adapted to return arrays\n",
    "from utils.angle_encoding_script import angle_encoding\n",
    "\n",
    "def pad_to_power_of_two(vec, n_qubits):\n",
    "    dim = 2 ** n_qubits\n",
    "    v = np.copy(vec)\n",
    "    if len(v) < dim:\n",
    "        v = np.pad(v, (0, dim - len(v)), mode='constant')\n",
    "    else:\n",
    "        v = v[:dim]\n",
    "    # normalize to unit norm (avoid divide-by-zero)\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        return v\n",
    "    return v / norm\n",
    "\n",
    "def generate_scrambled_dataset(\n",
    "    data,                     # numpy array (N, C, H, W) or (N, H, W)\n",
    "    sample_indices=None,      # list/array of indices to augment (None -> use all)\n",
    "    num_scrambles=4,\n",
    "    n_qubits=8,\n",
    "    depth=8,\n",
    "    seed=42,\n",
    "    shared_unitaries=True\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N = data.shape[0]\n",
    "    if sample_indices is None:\n",
    "        sample_indices = np.arange(N)\n",
    "    encoded_images = []            # original encoded (float arrays)\n",
    "    scrambled_versions_all = []    # list of lists: for each sample -> [scr1, scr2, ...]\n",
    "    # prepare shared scramblers if requested\n",
    "    if shared_unitaries:\n",
    "        print(f\"Generating a set of {num_scrambles} shared unitaries\")\n",
    "        seeds = [int(rng.integers(0, 1e9)) for _ in range(num_scrambles)]\n",
    "        shared_scramblers = [\n",
    "            lambda vec, s=s: fast_haar_scramble(vec, n_qubits=n_qubits, depth=depth, seed=int(s))\n",
    "            for s in seeds\n",
    "        ]\n",
    "    else:\n",
    "        shared_scramblers = None\n",
    "\n",
    "    for s_idx in sample_indices:\n",
    "        # pull single-channel image and flatten to vector\n",
    "        img = data[s_idx]\n",
    "        if img.ndim == 3 and img.shape[0] == 1:\n",
    "            img2d = img[0]\n",
    "        elif img.ndim == 2:\n",
    "            img2d = img\n",
    "        else:\n",
    "            # if multi-channel, take first channel\n",
    "            img2d = img[0] if img.ndim == 3 else img.squeeze()\n",
    "\n",
    "        # Option A: angle_encoding expects an image and wires â€” we will generate a numeric vector\n",
    "        # Many of your encoders transform image -> flattened vector representation. We'll call angle_encoding\n",
    "        # that returns a numeric vector only if your implementation supports it. Otherwise we will\n",
    "        # use the flattened pixel vector as fallback.\n",
    "        try:\n",
    "            # angle_encoding in your repo appears to be a function that encodes into a qnode; but here we just produce a flattened vector\n",
    "            encoded_flat = img2d.flatten().astype(np.float32)\n",
    "        except Exception:\n",
    "            encoded_flat = img2d.flatten().astype(np.float32)\n",
    "\n",
    "        # pad to 2^n_qubits dimension\n",
    "        flat_encoded = pad_to_power_of_two(encoded_flat, n_qubits)\n",
    "        encoded_images.append(flat_encoded)\n",
    "\n",
    "        if shared_unitaries:\n",
    "            scrambles = [scr(flat_encoded) for scr in shared_scramblers]\n",
    "        else:\n",
    "            scrambles = [\n",
    "                fast_haar_scramble(flat_encoded, n_qubits=n_qubits, depth=depth, seed=seed + s_idx * num_scrambles + k)\n",
    "                for k in range(num_scrambles)\n",
    "            ]\n",
    "\n",
    "        scrambled_versions_all.append(scrambles)\n",
    "\n",
    "    return np.array(encoded_images), scrambled_versions_all\n",
    "\n",
    "# quick test (small)\n",
    "encs, scrs = generate_scrambled_dataset(data, sample_indices=np.arange(min(10, data.shape[0])),\n",
    "                                       num_scrambles=2, n_qubits=n_qubits, depth=scramble_depth,\n",
    "                                       seed=seed, shared_unitaries=shared_unitaries)\n",
    "print(\"encoded shape:\", encs.shape, \"scrambles samples:\", len(scrs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "168fe62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a set of 4 shared unitaries\n",
      "Dataset size: 100 Example shapes: torch.Size([16, 1, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\realc\\AppData\\Local\\Temp\\ipykernel_2644\\2408876279.py:55: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Copy.cpp:305.)\n",
      "  cond = torch.from_numpy(scr_n).float()\n"
     ]
    }
   ],
   "source": [
    "class QGConditionalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that yields:\n",
    "      - x_input: possibly noised input to the model (e.g., for denoising or forward diffusion)\n",
    "      - cond: conditioning data (we'll use one scrambled augmentation per sample)\n",
    "      - target: the original image (supervision)\n",
    "    For simplicity we keep everything as torch.float32 in [0,1] as channel-first tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, originals, scrambled_lists, use_concat_condition=False):\n",
    "        # originals: numpy array (N, C, H, W)\n",
    "        # scrambled_lists: list length N where each element is list of scrambled vectors (num_scrambles)\n",
    "        self.orig = originals\n",
    "        self.scrambled_lists = scrambled_lists\n",
    "        self.N = len(originals)\n",
    "        self.use_concat_condition = use_concat_condition\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.orig[idx].astype(np.float32)  # (C, H, W)\n",
    "        # choose one scrambled augmentation at random\n",
    "        scr_list = self.scrambled_lists[idx]\n",
    "        # scrambled vectors are 1D; we will reshape them back into an image-like shape if desired.\n",
    "        scr_vec = scr_list[np.random.randint(0, len(scr_list))]\n",
    "        # re-expand scrambled vector to image shape (C, H, W) via simple tiling or reshape if appropriate\n",
    "        # Here we use a simple approach: reshape to (H, W) if dim matches, else tile\n",
    "        C, H, W = img.shape\n",
    "        scr_len = scr_vec.shape[0]\n",
    "        if scr_len == H * W:\n",
    "            scr_img = scr_vec.reshape((H, W))\n",
    "            scr_img = scr_img[None, :, :]  # add channel\n",
    "        else:\n",
    "            # tile/resize the vector into image shape (simple approach)\n",
    "            scr_img = np.tile(scr_vec[:H*W], int(np.ceil((H*W)/len(scr_vec))))[:H*W].reshape(H, W)[None, :, :]\n",
    "\n",
    "        # Normalize scrambled image into roughly same scale as original if needed\n",
    "        # We'll min-max normalize both to [0,1] for training stability (tune if your models expect different)\n",
    "        def normalize_arr(a):\n",
    "            amin = a.min()\n",
    "            amax = a.max()\n",
    "            if amax - amin <= 1e-8:\n",
    "                return np.zeros_like(a)\n",
    "            return (a - amin) / (amax - amin)\n",
    "\n",
    "        img_n = normalize_arr(img)\n",
    "        scr_n = normalize_arr(scr_img)\n",
    "\n",
    "        # Decide input_x (for conditional/diffusion model): many FM approaches add noise during training.\n",
    "        # Here we give the base (could add gaussian noise if you want).\n",
    "        x_input = img_n.copy()\n",
    "\n",
    "        # as torch tensors\n",
    "        x_input = torch.from_numpy(x_input).float()\n",
    "        cond = torch.from_numpy(scr_n).float()\n",
    "        target = torch.from_numpy(img_n).float()\n",
    "\n",
    "        # optionally prepare a concatenated input (2*C, H, W) if model doesn't accept an explicit condition argument\n",
    "        if self.use_concat_condition:\n",
    "            concat_input = torch.cat([cond, x_input], dim=0)  # (2*C, H, W)\n",
    "            return concat_input, cond, target\n",
    "\n",
    "        return x_input, cond, target\n",
    "\n",
    "# Build dataset (choose using first N samples)\n",
    "num_use = min(data.shape[0], num_samples_for_scramble)\n",
    "sample_indices = np.arange(num_use)\n",
    "encoded_images, scrambled_lists = generate_scrambled_dataset(data, sample_indices=sample_indices,\n",
    "                                                            num_scrambles=num_scrambles,\n",
    "                                                            n_qubits=n_qubits,\n",
    "                                                            depth=scramble_depth,\n",
    "                                                            seed=seed,\n",
    "                                                            shared_unitaries=shared_unitaries)\n",
    "\n",
    "# We pass the original images aligned with scrambled augmentations:\n",
    "orig_subset = data[:num_use]  # (N, C, H, W)\n",
    "dataset = QGConditionalDataset(orig_subset, scrambled_lists, use_concat_condition=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "print(\"Dataset size:\", len(dataset), \"Example shapes:\", next(iter(dataloader))[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITION HERE\n",
    "input_dim = 2 ** n_qubits\n",
    "output_dim = input_dim\n",
    "\n",
    "# Number of conditioning unitaries (= num_scrambles)\n",
    "num_unitaries = num_scrambles     \n",
    "\n",
    "# -------------------------\n",
    "# Base quantum model\n",
    "# -------------------------\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "base_model = QuantumDiffusionModel(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    n_qubits=n_qubits,\n",
    "    n_layers=n_layers\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Conditional wrapper for unitary indices\n",
    "# ============================================================\n",
    "\n",
    "class ConditionedQD(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds conditioning by injecting a learned embedding representing \n",
    "    which Haar unitary was used to scramble the vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, num_unitaries, input_dim, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.embed = nn.Embedding(num_unitaries, embed_dim)\n",
    "        self.project = nn.Linear(embed_dim, input_dim)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x_vec, unitary_idx):\n",
    "        \"\"\"\n",
    "        x_vec:         (batch, input_dim)\n",
    "        unitary_idx:   (batch,) integer index of which scrambler was used\n",
    "        \"\"\"\n",
    "        emb = self.embed(unitary_idx)     # (B, embed_dim)\n",
    "        delta = self.project(emb)         # (B, input_dim)\n",
    "        delta = self.act(delta)           # small bounded perturbation\n",
    "\n",
    "        x_cond = x_vec + delta            # conditioned input\n",
    "\n",
    "        out = self.base(x_cond)           # quantum model forward pass\n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate conditional model\n",
    "embed_dim = 64\n",
    "cond_model = ConditionedQD(\n",
    "    base_model=base_model,\n",
    "    num_unitaries=num_unitaries,\n",
    "    input_dim=input_dim,\n",
    "    embed_dim=embed_dim\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(cond_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b56232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGVectorDataset(Dataset):\n",
    "    def __init__(self, encoded_images, scrambled_lists):\n",
    "        self.encoded = encoded_images                    # shape (N, input_dim)\n",
    "        self.scrambled = scrambled_lists                 # list of lists\n",
    "        self.N = len(encoded_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_vec = self.encoded[idx]                       # original vector\n",
    "\n",
    "        # choose random unitary\n",
    "        unitary_idx = np.random.randint(0, len(self.scrambled[idx]))\n",
    "        target_vec = x_vec                              # reconstruction target\n",
    "\n",
    "        return (\n",
    "            torch.tensor(x_vec, dtype=torch.float32),\n",
    "            torch.tensor(unitary_idx, dtype=torch.long),\n",
    "            torch.tensor(target_vec, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "# build the vector dataset\n",
    "vec_dataset = QGVectorDataset(encoded_images, scrambled_lists)\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "val_fraction = 0.1\n",
    "val_size = int(len(vec_dataset) * val_fraction)\n",
    "train_size = len(vec_dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(vec_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d229446",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 341, 12]' is invalid for input of size 65536",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m xb, yb, idxb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device), yb\u001b[38;5;241m.\u001b[39mto(device), idxb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcond_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, yb)\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 50\u001b[0m, in \u001b[0;36mConditionedQD.forward\u001b[1;34m(self, x_vec, unitary_idx)\u001b[0m\n\u001b[0;32m     46\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(delta)           \u001b[38;5;66;03m# small bounded perturbation\u001b[39;00m\n\u001b[0;32m     48\u001b[0m x_cond \u001b[38;5;241m=\u001b[39m x_vec \u001b[38;5;241m+\u001b[39m delta            \u001b[38;5;66;03m# conditioned input\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cond\u001b[49m\u001b[43m)\u001b[49m           \u001b[38;5;66;03m# quantum model forward pass\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GSOC\\utils\\quantum_diffusion.py:39\u001b[0m, in \u001b[0;36mQuantumDiffusionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 39\u001b[0m     x_patched \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_qubits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantum_layer(x_patched[:, p]) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_patches)]\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 341, 12]' is invalid for input of size 65536"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    cond_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, idxb, yb in train_loader:\n",
    "        xb, yb, idxb = xb.to(device), yb.to(device), idxb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = cond_model(xb, idxb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    cond_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, idxb, yb in val_loader:\n",
    "            xb, yb, idxb = xb.to(device), yb.to(device), idxb.to(device)\n",
    "            val_loss += criterion(cond_model(xb, idxb), yb).item() * xb.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train: {avg_train_loss:.5f} | Val: {avg_val_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   TRAINING LOOP (clean, with validation)\n",
    "# ============================================================\n",
    "\n",
    "# Split into train/val\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "val_fraction = 0.1\n",
    "val_size = int(len(vec_dataset) * val_fraction)\n",
    "train_size = len(vec_dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(vec_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {train_size}, Validation samples: {val_size}\")\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ------------------------------------\n",
    "    # Training\n",
    "    # ------------------------------------\n",
    "    cond_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, unitary_idxb, yb in train_loader:\n",
    "        xb = xb.to(device)                  # (batch, input_dim)\n",
    "        yb = yb.to(device)                  # (batch, input_dim)\n",
    "        unitary_idxb = unitary_idxb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = cond_model(xb, unitary_idxb)  # conditioned forward pass\n",
    "        loss = criterion(out, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Validation\n",
    "    # ------------------------------------\n",
    "    cond_model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, unitary_idxb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            unitary_idxb = unitary_idxb.to(device)\n",
    "\n",
    "            out = cond_model(xb, unitary_idxb)\n",
    "            val_loss += criterion(out, yb).item() * xb.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Logging\n",
    "    # ------------------------------------\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
    "          f\"- Train: {avg_train_loss:.5f} | Val: {avg_val_loss:.5f}\")\n",
    "\n",
    "\n",
    "    # Optional checkpointing\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(cond_model.state_dict(),\n",
    "                   os.path.join(out_dir, f\"cond_model_ep{epoch+1}.pt\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}